import streamlit as st
import pandas as pd
from deep_translator import GoogleTranslator
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from PIL import Image

from sentence_transformers import SentenceTransformer, util
# import gensim 
# from gensim.models import Word2Vec




# C√†i ƒë·∫∑t giao di·ªán trang
st.set_page_config(layout="wide")

# Load ·∫£nh
logo1 = Image.open("E:\\SIC\\app\\logo_dhm.png")
logo2 = Image.open("E:\\SIC\\app\\CNTT.png")
logo3 = Image.open("E:\\SIC\\app\\SIC.jpg")

# Hi·ªÉn th·ªã logo theo h√†ng ngang
col1, col2, col3 = st.columns(3)

with col1:
    
    st.markdown(
        "<div style='text-align: center; font-size:22px; margin-top:10px;'>"
        "Tr∆∞·ªùng ƒê·∫°i h·ªçc M·ªü Th√†nh ph·ªë H·ªì Ch√≠ Minh"
        "</div>", unsafe_allow_html=True)
    st.image(logo1, use_container_width=True)
with col2:
    
    st.markdown(
        "<div style='text-align: center; font-size:22px; margin-top:10px;'>"
        "Khoa C√¥ng ngh·ªá Th√¥ng tin"
        "</div>", unsafe_allow_html=True)
    st.image(logo2, use_container_width=True)
with col3:
    
    st.markdown(
        "<div style='text-align: center; font-size:22px; margin-top:10px;'>"
        "Samsung Innovation Campus"
        "</div>", unsafe_allow_html=True)
    st.image(logo3, use_container_width=True)
# Th√™m ti√™u ƒë·ªÅ ch√≠nh
st.markdown("<h2 style='text-align: center; color: #4a90e2;'>H·ªá th·ªëng G·ª£i √Ω Vi·ªác l√†m</h2>", unsafe_allow_html=True)
st.markdown("<h4 style='text-align: center; color: gray;'>Th·ª±c hi·ªán b·ªüi Nh√≥m 3 </h4>", unsafe_allow_html=True)



# Load d·ªØ li·ªáu
@st.cache_data
def load_data():
    return pd.read_excel("E:\\SIC\\app\\jobs_clean.xlsx")

df = load_data()
#---------------------
import joblib

rf_model = joblib.load("E:/SIC/app/career_rf_model.pkl")
vectorizer = joblib.load("E:/SIC/app/tfidf_vectorizer.pkl")
label_encoder = joblib.load("E:/SIC/app/label_encoder.pkl")

#--------

# ========== 1. Ch·ªçn ng√†nh ngh·ªÅ ==========
# industries = df["nganh_nghe_clean"].dropna().unique().tolist()
# industry_choice = st.selectbox("üìå Ch·ªçn ng√†nh ngh·ªÅ:", sorted(industries))

# df_industry = df[df["nganh_nghe_clean"] == industry_choice].copy()
df_industry = df.copy()


# L·∫•y to√†n b·ªô d·ªØ li·ªáu t·ª´ c·ªôt khu_vuc
all_locations = df["khu_vuc"].dropna().astype(str).tolist()

# T√°ch theo d·∫•u ph·∫©y r·ªìi gom l·∫°i th√†nh 1 list duy nh·∫•t
split_locations = []
for loc in all_locations:
    split_locations.extend([x.strip() for x in loc.split(",")])

# L·∫•y unique + sort
tat_ca = sorted(set(split_locations))

# Checkbox ch·ªçn t·∫•t c·∫£
chon_tat_ca = st.checkbox("‚úÖ Ch·ªçn t·∫•t c·∫£ khu v·ª±c", value=False)

if chon_tat_ca:
    khu_vuc_chon = st.multiselect("üìç Ch·ªçn khu v·ª±c l√†m vi·ªác:", options=tat_ca, default=tat_ca)
else:
    khu_vuc_chon = st.multiselect("üìç Ch·ªçn khu v·ª±c l√†m vi·ªác:", options=tat_ca)

#st.write("‚úÖ Khu v·ª±c ƒë√£ ch·ªçn:", khu_vuc_chon)

# L·ªçc theo khu v·ª±c ƒë√£ ch·ªçn
pattern = '|'.join(khu_vuc_chon)
df_location = df_industry[df_industry["khu_vuc"].fillna("").str.contains(pattern, case=False)].copy()

# ========== 3. Ch·ªçn c·∫•p b·∫≠c ==========
cap_bac_list = df_location["cap_bac_standardized"].dropna().unique().tolist()
cap_bac_choices = st.multiselect("üè∑Ô∏è Ch·ªçn c·∫•p b·∫≠c:", sorted(cap_bac_list), default=cap_bac_list)

# L·ªçc d·ªØ li·ªáu cu·ªëi c√πng theo khu v·ª±c + c·∫•p b·∫≠c
df_final = df_location[df_location["cap_bac_standardized"].isin(cap_bac_choices)].copy()

# ========== 4. Nh·∫≠p kinh nghi·ªám & k·ªπ nƒÉng ==========
experience = st.number_input("üéì S·ªë nƒÉm kinh nghi·ªám:", min_value=0, max_value=30, value=1)
skills_vi = st.text_input("üõ† Nh·∫≠p k·ªπ nƒÉng (c√°ch nhau b·ªüi d·∫•u ph·∫©y):")

# ========== 5. D·ªãch k·ªπ nƒÉng ==========
def translate_vi_to_en(text):
    if text.strip() == "":
        return ""
    try:
        return GoogleTranslator(source='vi', target='en').translate(text)
    except Exception as e:
        st.error(f"L·ªói d·ªãch: {e}")
        return text
if st.button("ü§ñ G·ª£i √Ω ng√†nh v√† c√¥ng vi·ªác ph√π h·ª£p"):
    if skills_vi.strip() == "":
        st.warning("‚ùó B·∫°n c·∫ßn nh·∫≠p k·ªπ nƒÉng.")
    elif not khu_vuc_chon:
        st.warning("‚ùó B·∫°n c·∫ßn ch·ªçn √≠t nh·∫•t m·ªôt khu v·ª±c.")
    elif not cap_bac_choices:
        st.warning("‚ùó B·∫°n c·∫ßn ch·ªçn c·∫•p b·∫≠c.")
    else:
        with st.spinner("üöÄ ƒêang x·ª≠ l√Ω..."):
            # ======= 1. D·ªãch k·ªπ nƒÉng v√† d·ª± ƒëo√°n ng√†nh =======
            skills_en = translate_vi_to_en(skills_vi)
            skill_vec = vectorizer.transform([skills_en])
            pred = rf_model.predict(skill_vec)
            predicted_career = label_encoder.inverse_transform(pred)[0]

            st.success(f"üåü Ng√†nh ngh·ªÅ ph√π h·ª£p: **{predicted_career}**")

            # ======= 2. L·ªçc c√¥ng vi·ªác theo ng√†nh, khu v·ª±c, c·∫•p b·∫≠c =======
            df_filtered = df.copy()
            pattern = '|'.join(khu_vuc_chon)
            df_filtered = df_filtered[df_filtered["khu_vuc"].fillna("").str.contains(pattern, case=False)]
            df_filtered = df_filtered[df_filtered["cap_bac_standardized"].isin(cap_bac_choices)]
            df_filtered = df_filtered[df_filtered["nganh_nghe_clean"] == predicted_career]

            if df_filtered.empty:
                st.warning("üò• Kh√¥ng t√¨m th·∫•y c√¥ng vi·ªác ph√π h·ª£p v·ªõi ng√†nh v√† ti√™u ch√≠ ƒë√£ ch·ªçn.")
            else:
                # ======= 3. T√≠nh similarity b·∫±ng Sentence-BERT =======
                model = SentenceTransformer('all-MiniLM-L6-v2')
                job_skills = df_filtered["ky_nang_combined"].fillna("").tolist()
                job_embeddings = model.encode(job_skills, convert_to_tensor=True)
                user_embedding = model.encode(skills_en, convert_to_tensor=True)

                sims = util.cos_sim(user_embedding, job_embeddings)[0].cpu().numpy()
                df_filtered["similarity_skills"] = sims

                # ======= 4. Ch·ªçn Top 10 c√¥ng vi·ªác ph√π h·ª£p =======
                top_jobs = df_filtered.sort_values(by="similarity_skills", ascending=False).head(10)

                # ======= 5. Hi·ªÉn th·ªã k·∫øt qu·∫£ =======
                st.subheader("‚úÖ Top 10 c√¥ng vi·ªác ph√π h·ª£p v·ªõi b·∫°n:")
                # T·∫°o c·ªôt ph·∫ßn trƒÉm t·ª´ similarity_skills
                top_jobs["muc_do_phu_hop(%)"] = (top_jobs["similarity_skills"] * 100).round(1).astype(str) + "%"
                # L·∫•y c√°c c·ªôt hi·ªÉn th·ªã
                top_jobs_display = top_jobs[[ "ten_cong_viec", "khu_vuc", "cap_bac_standardized", "muc_do_phu_hop(%)", "nganh_nghe_clean", "link"]].rename(columns={
                    "ten_cong_viec": "T√™n c√¥ng vi·ªác","khu_vuc": "Khu v·ª±c","cap_bac_standardized": "C·∫•p b·∫≠c","muc_do_phu_hop(%)": "M·ª©c ƒë·ªô ph√π h·ª£p","nganh_nghe_clean": "Ng√†nh ngh·ªÅ",
                    "link": "Link tuy·ªÉn d·ª•ng"})
                # Th√™m c·ªôt STT th·ªß c√¥ng
                top_jobs_display.reset_index(drop=True, inplace=True)
                top_jobs_display.insert(0, "STT", range(1, len(top_jobs_display) + 1))
                # Hi·ªÉn th·ªã m√† kh√¥ng hi·ªÉn th·ªã index m·∫∑c ƒë·ªãnh
                st.dataframe(top_jobs_display, use_container_width=True, hide_index=True)

# #-------------------------
# if st.button("üìä D·ª± ƒëo√°n ng√†nh ngh·ªÅ ph√π h·ª£p (ML)"):
#     if skills_vi.strip() == "":
#         st.warning("B·∫°n c·∫ßn nh·∫≠p k·ªπ nƒÉng.")
#     else:
#         skills_en = translate_vi_to_en(skills_vi)
#         skill_vec = vectorizer.transform([skills_en])
#         pred = rf_model.predict(skill_vec)
#         predicted_career = label_encoder.inverse_transform(pred)[0]

#         st.success(f"üåü Ng√†nh ngh·ªÅ ƒë∆∞·ª£c g·ª£i √Ω: **{predicted_career}**")
# #-------------------


# # ========== 6. T√≠nh TF-IDF + Cosine Similarity ==========
# if st.button("üîé T√¨m vi·ªác theo k·ªπ nƒÉng"):
#     if df_final.empty:
#         st.warning("Kh√¥ng t√¨m th·∫•y c√¥ng vi·ªác ph√π h·ª£p v·ªõi khu v·ª±c v√† c·∫•p b·∫≠c ƒë√£ ch·ªçn.")
#     else:
#         skills_en = translate_vi_to_en(skills_vi)
#         st.success(f"üëâ K·ªπ nƒÉng ƒë√£ d·ªãch sang EN: **{skills_en}**")

#         # # TF-IDF cho k·ªπ nƒÉng
#         # job_skills = df_final["ky_nang_combined"].fillna("").tolist()
#         # tfidf = TfidfVectorizer(max_features=5000)
#         # tfidf_matrix = tfidf.fit_transform(job_skills)
#         # user_vec = tfidf.transform([skills_en])
#         # sims = cosine_similarity(user_vec, tfidf_matrix).flatten()

#         # df_final["similarity_skills"] = sims
#         # top_jobs = df_final.sort_values(by="similarity_skills", ascending=False).head(10)

#         # st.subheader("‚úÖ Top 10 c√¥ng vi·ªác ph√π h·ª£p v·ªõi k·ªπ nƒÉng")
#         # st.dataframe(top_jobs[[
#         #     "ten_cong_viec", "khu_vuc", "cap_bac_standardized", 
#         #     "similarity_skills", "nganh_nghe_clean","link"
#         # ]])

#         #Load m√¥ h√¨nh Sentence-BERT
#         with st.spinner("üîç ƒêang t√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng..."):
#             model = SentenceTransformer('all-MiniLM-L6-v2')

#             job_skills = df_final["ky_nang_combined"].fillna("").tolist()
#             job_embeddings = model.encode(job_skills, convert_to_tensor=True)
#             user_embedding = model.encode(skills_en, convert_to_tensor=True)

#             sims = util.cos_sim(user_embedding, job_embeddings)[0].cpu().numpy()
#             df_final["similarity_skills"] = sims
#             top_jobs = df_final.sort_values(by="similarity_skills", ascending=False).head(10)

#         st.subheader("‚úÖ Top 10 c√¥ng vi·ªác ph√π h·ª£p v·ªõi k·ªπ nƒÉng")
#         st.dataframe(top_jobs[[ 
#             "ten_cong_viec", "khu_vuc", "cap_bac_standardized", 
#             "similarity_skills", "nganh_nghe_clean", "link"
#         ]])

#         # #w2_vec
#         # # Vector h√≥a k·ªπ nƒÉng ng∆∞·ªùi d√πng b·∫±ng Word2Vec
#         # user_vec = sentence_vector(skills_en, word_vectors)

#         # job_skills = df_final["ky_nang_combined"].fillna("").tolist()
#         # job_vecs = [sentence_vector(s, word_vectors) for s in job_skills]

#         # # T√≠nh cosine similarity
#         # def cosine_sim(v1, v2):
#         #     if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:
#         #         return 0
#         #     return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

#         # sims = [cosine_sim(user_vec, job_vec) for job_vec in job_vecs]
#         # df_final["similarity_skills"] = sims

#         # top_jobs = df_final.sort_values(by="similarity_skills", ascending=False).head(10)
#         # st.subheader("‚úÖ Top 10 c√¥ng vi·ªác ph√π h·ª£p v·ªõi k·ªπ nƒÉng (Word2Vec)")
#         # st.dataframe(top_jobs[[
#         #     "ten_cong_viec", "khu_vuc", "cap_bac_standardized", 
#         #     "similarity_skills", "nganh_nghe_clean","link"
#         # ]])
    

